This chapter delves into the development of 5 prototypes, each portraying a different type of behaviour for hands in VR. These different prototypes approach the problem of realism vs sense of embodiment in different ways and to different degrees. In the following sections, we will present a categorization of approaches that can be taken when implementing hands in VR, what effects these different approaches have and their pros and cons.\\

All 5 prototypes have been developed using the Unity engine and the HTC Vive with default controllers for input. The space we've explored during the development of the prototypes have been restricted by these choices. Unity has a set of APIs which we as users of Unity have available to us. These APIs have shaped what has been possible for us during implementation of the prototypes\footnote{Unity's scripting reference can be found online on their website \parencite{UnityScriptingReference2017}.}. Furthermore, building the prototypes around using the HTC Vive's controllers as the input method makes our methods and approaches more applicable to hardware that has the same affordances as these. As for the input used to control the hands the position and orientation input is gathered from tracking the controllers and the controllers' trigger buttons are used to indicate how much the fingers should bend (how closed the hand should be).

These three inputs are the base for different categories of approaches. Each of these inputs can be filtered, by which is meant that the input can be modified or skipped. Filtering on one of the inputs can be seen as using that input as the parameter for a function, which returns a new result. A hand can have a filter for either none of the inputs or one of the inputs or more. Different filtering combinations will give the hand a different behaviour in the virtual world and might result in the hand seeming more realistic when interacting with its environment.

The 5 hand prototypes are named as follows: \textit{Rigid hand}, \textit{Sliding rigid hand}, \textit{Finger rigid hand}, \textit{Physics hand} and \textit{Rotation hand}. They differ in the way they filter on the player's inputs which gives them a different feel. The detail of which filters and their implementation will be given in the sections below.

\section{Categorization of approaches}
\label{sec:categorizationOfApproaches}
The three player inputs mentioned above (position, orientation and how closed the hand should be) are the base of the categorization of approaches. Each of these inputs can be filtered, by which is meant that the input can be modified or skipped. Filtering on one of the inputs can be seen as using the input as the parameter for a function, which returns a new result which will be used instead of that input. A hand can be implemented with filters on several inputs at once and can have different filters depending on the context. Different filterer combinations will give a different behaviour for the hand in the virtual world and can result in the hand seeming more realistic when interacting with its environment.

\begin{table}[H]
\centering
\caption{Filter variable combinations.}
\label{tab:filterVariableCombinations}
\begin{tabular}{C{2cm}C{2cm}C{2cm}}
Position & Rotation & Finger position \\ \midrule \midrule
				&					&					\\ \midrule
\Large X	&					&					\\ \midrule
				& \Large X	& 		                \\ \midrule
				&					& \Large X     \\ \midrule
\Large X	& \Large X	&					\\ \midrule
\Large X 	&					& \Large X	\\ \midrule
				& \Large X	& \Large X	\\ \midrule
\Large X 	& \Large X 	& \Large X
\end{tabular}
\end{table}

\subsection{Position filtering}
\label{subsec:categoryPositionFiltering}
A position filter using the definition above is a function, which when enabled, takes the player's positional input from a controller and returns a new value to be used in its stead. This means that the position of the hand in the virtual world will deviate from the controller position.\\

One of our hypotheses is that it's possible to increase the player's sense of embodiment by simulating more realistic behaviour for hands around objects in the world. One of the first steps here is to not allow the hands to penetrate objects. By using position filtering to deviate the hand from the controller position when trying to penetrate an object, the hand can interact more realistically with the world.

The following two figures show an image sequence of a hand with no input filtering and an image sequence with a hand that uses position filtering. The second figure shows that the hand is stopping when it reaches the object, whereas in the first figure the hand moves through the object.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Sequences/FiltersNone/Seq_FiltersNone.png}
\caption{Sequence showing hand without filters entering obstacle.}
\label{fig:filtersNone}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Sequences/FiltersPosition/Seq_FiltersPosition.png}
\caption{Sequence showing hand filtering on position. Notice the controller continuing to move into the obstacle.}
\label{fig:filtersPosition}
\end{figure}

Implementing position filtering for hands is about determining how to make the hand follow the controller. All of our 5 prototypes use position filtering, but the implementations of the filters differ. The first distinction is between physics bases and non-physics based methods. The physics hand is the only one of our prototypes that falls into the first category. The way the physics hand is moved towards the controller is to set the velocity each frame so that the hand will reach the controller within the next frame using the physics system. Using velocities and the physics system to move the hand has several positive aspects. The main gain of using the physics system to move is that it adheres to the rules of the system, including collisions, which means that the hand will not penetrate objects in the world which have colliders. Using the physics system to avoid penetration is very efficient since it's one of the core features of the Unity engine. One of the downsides of using the physics system to handle collisions and more is that we as the developers have no direct control over the inner workings of the system and therefore lose some control of how the hand behaves.

As for the prototypes that are implemented using non-physics based methods, their filters are all based on three different approaches to stop the hands from penetrating objects: Sweep and place, skip movement in direction and depenetration. These three methods fall into two categories; pre-collision correction methods and post-collision correction methods. The first two methods fall into the pre-collision correction category, because they try to avoid penetration of objects before it happens, whereas depenetration, which falls into the post-collision correction category will correct the penetration after it has happened. In the different prototypes these three methods are used alone or in combination to create the behaviour wanted from that prototype. The simplest of these prototypes is the Rigid hand. Here only the depenetration comes into play. Depenetrating the hand from an object means to find the direction and distance in which to move the colliders of the hand in order for the hand and the object not to be overlapping anymore. The direction is to the closest surface and the distance is the distance needed to move for the collider to be outside the object's colliders. When moving the hands in this prototype, their position is simply set to the target position on the controller each frame after which they are depenetrated from any objects they would be overlapping with. Using this method creates very smooth interactions with static objects, but have drawbacks for interacting with dynamic objects (\textbf{\textit{EXPLAIN THESE DRAWBACKS}}). Since the hand is always depenetrated to the closest surface there is also the drawback of the hand jumping between surfaces, when the controller is places inside a static object. If in one frame the hand is closer to one surface and another surface in the next frame, depenetrating the hand from the object will lead it to move towards another surface creating a jump.

The problem of jumping between surfaces, when using depenetration is what the Sliding rigid hand is trying to solve. They employ the other two methods, Sweep and place and skip movement in direction, in order to combat the jumping between surfaces. The main idea behind this prototype is to try avoiding penetrating an object altogether or if it happens to only penetrate a small amount so that depenetrating the hand would always select the same surface from where it entered the object. Sweeping is about taking the hand and calculating how far in a direction it can move before it hits something. Before moving the hand to the controller position we can sweep and see if it would hit an object on the way. If an object was hit then the hand can be placed there instead of placing it at the controller. This way the hand will not penetrate the object although the controller is inside or on the opposite side of it. In Unity sweeping in a direction will not detect an object if the hand already touches the object. Therefore, in subsequent frames from placing the object on the surface, another method is needed for the hand to not penetrate the object. Skipping movement in a direction is one way to alleviate this problem. When the hand is touching the surface of an object, it shouldn't move in the direction towards the surface, because that would lead the hand to penetrate the object. Therefore, we skip all movement along that direction, essentially allowing the hand to only move on a plane. Moving on a plane works wonders, when interacting with cubes, but has its downsides, when dealing with objects with other shapes. Sliding on a plane might lead the hand to penetrate parts of an object that point out or penetrate other objects. Depenetration helps correct the hands in the situations, where they end up inside an object because correcting in one way leads to errors some place else. These three methods combined should lead to hands that don't penetrate objects, but also doesn't jump between surfaces due to depenetration.

The two remaining prototypes, the Finger rigid hand and the Rotation hand, both use the methods mentioned above to achieve their position filtering. The Finger rigid hand use the exact same position filtering method as the Rigid hands and the Rotation hand use a filter similar to the Sliding rigid hand with the exception that it doesn't do any depenetration.

\subsection{Rotation filtering}
\label{subsec:categoryRotationFiltering}
\textbf{Questions to answer in this section:}
\begin{itemize}
\setlength\itemsep{-0.1cm}
\item What does it mean to filter the player's rotational input?
\item Why do we want to filter the player's rotational input?
\begin{itemize}
\setlength\itemsep{-0.1cm}
\item Reduce position filtering.
\item Display player's intentions.
\item Diagetically communicate available interactions.
\item Allows for more detailed hand animation depending on available interactions. Control?
\item Mention stiffness with only position filtering
\end{itemize}
\end{itemize}

\textbf{What does it mean to filter the player's rotational input?}\\
To use a rotation filter is to deviate the hands rotation from the current orientation of the controller. In certain contexts it can be beneficial filter on rotation on order for the hand to seem more alive and realistic. Hands can adapt their rotation in several ways and different approaches might be taken depending on the context. When approaching a wall with their hand a player's intention in the real world might be to place their hand flat on the wall (palm-first). This behaviour can be implemented using a rotation filter, which takes effect when the hand is approaching a surface.

\textbf{Why do we want to filter the player's rotational input?}\\
Rotation filtering can be used to display what is assumed to be the player's intention when they interact with the world. One example could be the intention of the player when they approach a wall with their hand. In this case a reasonable assumption would be that the player's intention is to rotate the hand so that the palm faces the wall (See image sequences below). Besides being useful when wanting to adapt the hand to the player's intentions, rotation filtering can also be used in order to reduce the amount of position filtering needed. This means that rotating the hand will allow the distance between the hand and the controller due to position adjusting to be reduced (See image sequence below).

\todo[inline]{use refs}
The first of the two figures below shows an image sequence of a hand using only rotation filtering and the second figure shows an image sequence of a hand using both position and rotation filtering where the rotation filtering is implemented as palm-first towards the surface.

\missingfigure[figwidth=15cm]{Image sequence: Rotation filtering}
\missingfigure[figwidth=15cm]{Image sequence: Position and rotation filtering}

\textbf{Questions to answer in this section:}
\begin{itemize}
\setlength\itemsep{-0.1cm}
\item How can we filter the player's rotational input?
\begin{itemize}
\setlength\itemsep{-0.1cm}
\item Manual rotation filtering when approaching obstacles.
\item Differentiation between object types and angles of approach.
\item Physics system.
\end{itemize}
\end{itemize}

\todo[inline]{Remember explicit vs implicit rotation filtering (explicit being where we completely control the rotation vs the for instance the physics system controlling it).}

\textbf{How can we filter the player's rotational input?}\\
Although all our hand prototypes use position filters, only 2 of our 5 hand prototypes (Physics hand and Rotation hand) use rotation filtering.

For the physics hand, we don't use any explicit rotation filtering. The physics system decides the rotation of the hand, when interacting with surfaces. While the controller is within an object, the physics hand can decide to rotate the hand to a flat position (either palm-up or palm-down) in order to reduce the position deviation between the hand and the controller. The rotation happens as a jump and not as a smooth movement towards the surface resulting in a less natural feel during the jump, but a somewhat realistic look afterwards. To improve upon this, explicit rotation filtering could have been added to smoothly rotate the hand to lay flat on the surface, which could remove the jump altogether, but at the same time introduce complexity to the implementation.

Contrary to the Physics hand, the Rotation hand uses explicit rotation filtering. The goal of the rotation filtering for this hand was to always approach a surface palm-first. The main example to explain this choice is the approach of a hand towards a wall. When a player wants to put their hand on a wall, which way would they want to do this? Having an open hand and moving the hand fingers first wouldn't make much sense. It would not give support if the reason for touching the wall was to lean against it, for instance. Rotating the hand so that the palm would be placed firmly on the surface of the wall would make more sense in this case. With this as the main idea behind the rotation, the implementation then had to support this in several cases. The easy case is when the player approaches the wall with an open hand and the palm being closer to the surface than the back of the hand. Depending on the distance to the surface we rotate the palm of the hand the rest of the way until it is directly facing the surface. Another case is what to do when the hand is closed into a fist. In this case, the player is probably trying to punch the wall, which means that the palm shouldn't be facing the wall. Testing for this case isn't necessarily hard in itself, but other cases pile on top of this, including what to do when approaching with the back of the hand, what to do when the player is rotating while the hand is touching the surface and more.

\subsection{Finger position filtering}
\label{subsec:categoryFingerFiltering}
\textbf{Questions to answer in this section:}
\begin{itemize}
\setlength\itemsep{-0.1cm}
\item What does it mean to filter the player's finger position input?
\item Why do we want to filter the player's finger position input?
\begin{itemize}
\setlength\itemsep{-0.1cm}
\item Reduce position filtering.
\item Display player's intention.
\end{itemize}
\end{itemize}

\textbf{What does it mean to filter the player's finger position input?}\\
Filtering the finger positions is about placing the finger tips in space relatively to the rest of the hand or put differently; stretching and bending the fingers. The fingers can be filtered as a group or individually and different contexts can determine different filters.

\textbf{Why do we want to filter the player's finger position input?}\\
Here, like with the rotation filtering, a certain amount of assumptions have to be made about what the player's intend is. When a player's hand is approaching an object that can be grabbed, the most common case might be that they are trying to grab the object. If this is the case, adjusting the fingers to form a grip could be a natural behaviour.

\missingfigure[figwidth=15cm]{Image sequence: Position and finger position filtering}

\textbf{Questions to answer in this section:}
\begin{itemize}
\setlength\itemsep{-0.1cm}
\item How can we filter the player's finger position input?
\begin{itemize}
\setlength\itemsep{-0.1cm}
\item Filtering to avoid obstacles.
\item Filtering to anticipate player intent.
\item Differentiation between object types and angles of approach.
\end{itemize}
\end{itemize}

\textbf{How can we filter the player's finger position input?}\\
Implementations of finger position filtering include using an animation system to animate the fingers together or individually to different poses and the use of an Inverse Kinematics (IK) system to infer finger pose from finger tip position and hand position and orientation.

\todo[inline]{One method is to move the fingers closer to an object, usually to indicate that an object is interactive. Another method is to move the fingers away from an object in order for them to avoid penetrating them.}

\section{The hand prototypes}
\label{sec:handPrototypes}
This section will go into depth about how each of the five hand prototypes have been implemented, mention the pros and cons of the different approaches and finally a step-by-step process for creating the behaviour of the prototype will be presented. The table below (\ref{tab:handPrototypes}) stands as a reminder of what filters each of the hand prototypes implements.

\begin{table}[H]
\centering
\caption{The hand prototypes and their filters.}
\label{tab:handPrototypes}
\begin{tabular}{C{2.8cm}|C{2cm}C{2cm}C{2cm}C{2cm}C{2cm}}
 & Rigid hand & Sliding rigid hand & Finger rigid hand & Physics hand & Rotation hand \\ \midrule
Position filter & \Large X & \Large X & \Large X & \Large X & \Large X\\ \midrule
Rotation filter & & & & \Large X & \Large X \\ \midrule
Finger position filter & & & \Large X & \Large X & \Large X 
\end{tabular}
\end{table}

All the hands have three thing in common. First the hands require that colliders are set up for their models. We use one box collider for the palm of the hand and one for each part of the fingers. It's important to make sure that the colliders are not able to collide and interact with each other. In Unity we made all colliders reside on a specific layer and made the physics system ignore interactions between objects within that layer. If this is not done, the hand might collide with itself and disallow fingers from bending etc. The second thing we've used for all hands is Unity's rigidbody component. This component lets the hands interact with the world through the physics system. In Unity a rigidbody component can either be kinematic or non-kinematic. A kinematic rigidbody is not affected by outside sources which means it's not affected collisions by, for instance. A non-kinematic rigidbody is affected by outside sources and will be affected by, for instance, gravity and collisions. All of our hands use kinematic rigidbodies with the exception of the physics hands. The last thing the hands all need is an animation to close the hand. Our animations consist of two frames; one where the hand is fully open and one where the hand is closed. We can blend between the two frames using the trigger value from the controller, which therefore controls how closed the hand is. Some of the hands have different considerations whether the hand is open or closed. For all hands we consider the hand closed, when the trigger value - which can be between 0 (not pressed) and 1 (fully pressed) - is equal to or greater than 0.5.

Another thing worth mentioning before continuing to the implementation details for each of the hand prototypes is what the target position and rotation of the hand is. The placement of the hand compared to the controller is a decision that impacts how the user will move the controller around to interact with objects and how the hand feels to use. We chose to place the hand compared to the controller in the virtual world in the same way the user's real hand is placed according to the controller in the real world. As shown in Figure \ref{fig:handControllerPlacement}, the right hand is placed to the right of the controller in a position that could grab and hold the controller. The left hand goes on the left side of the controller but is otherwise the same as the right hand. Our reasoning behind this choice was to map the hand as closely to the position of the user's hands in the real world. From now on, when the controller's position and orientation is mentioned, it's the position and orientation the hands display in this image that we're referring to.

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{HandControllerPlacementBoth.png}
\caption{Two images showing the placement of the hands compared to the controllers.}
\label{fig:handControllerPlacement}
\end{figure}

\subsection{The Rigid hand}
\label{subsec:rigidHand}
As mentioned above in section \ref{subsec:categoryPositionFiltering}, the Rigid hand moves by setting the rigidbody's position and rotation to the selected position and rotation on the controller. In the case of moving the hand freely around without any collisions occurring the hand just follows the controller. In the case where the hand has penetrated one or more objects we will depenetrate it from the object. To detect if the hand needs to be depenetrated, we need to check if there is an overlap between the hand's colliders and any other colliders in the world. To reduce the number of colliders to check, we first find all colliders within a radius of the hand. Having these colliders we can now check if any of the hand's colliders have penetrated one or more of them. Unity's Physics API allows us to compute the penetration\footnote{To compute the penetration we use Unity's $Physics.ComputePenetration$ method.} between two colliders, which gives us the depenetration direction and distance. Iterating over all colliders in both the hand and the penetrated object, we can use the information gathered to move the hand out of the object, correcting the penetration error.

\begin{figure}[H]
\centering
\small
\begin{enumerate}[noitemsep]
\item Move the hand's rigidbody to the target controller.
\item Orient the hand's rigidbody compared to the target controller.
\item Find all colliders within a small radius of the hand.
\item Compute if any of the found colliders are penetrated by one of the hand's colliders.
\item Depenetrate the hand based on the penetration information computed in the previous step.
\end{enumerate}
\caption{Step-by-step process for placing the Rigid hand.}
\label{fig:stepByStepRigidHand}
\end{figure}

\todo[inline]{Write about the good and bad cases and pros and cons for this hand prototype. Like a mini-conclusion.}

\subsection{The Sliding rigid hand}
\label{subsec:slidingRigidHand}
The Sliding rigid hand is state based with two different states: Outside and touching. The two states let the hand behave differently and are used in different contexts. Each frame one of these states will be executed. The state which will be executed will be determined in the beginning of the frame depending on information from the last state executed and from the current context.

The outside state is executed, when the hand is not touching an object. When executed the hand is first rotated to the orientation of the controller target. After rotating the hand we sweep towards the controller, checking if there's any object in between the hand and the controller. If no obstacle was in the way, the hand is moved towards the controller. If an object is blocking the path then we position the hand so that it is touching the object. Unity's $Rigidbody.SweepTest$ method gives us the necessary information to do so: direction and distance to the object that was blocking the path.

There is only one transition from the outside state to the touching state; if the sweep detected an object in between the hand and the controller, while executing the outside state, the hand will have been placed on the surface of the object. In this case we will switch to the touching state. If nothing was obstructing the path the hand stays in the outside state.

As with the outside state, the first thing that happens in the touching state is that we rotate the hand so that it matches the orientation target on the controller. Because we rotated the hand, while it was touching an object we need to check two things; if the rotation has caused the hand to be removed from touching the surface of the object or if the rotation has resulted in the hand having penetrated the object. First, we check if the hand is inside an obstacle and depenetrates it if necessary. This is done in the same fashion as mentioned for the Rigid hand (Section \ref{subsec:rigidHand}). If the hand wasn't inside any object we try to sweep towards the controller position to check if the hand is currently not touching any object. If the sweep resulted in hitting an object in the path towards the controller, we move towards the location hit on that object. This works like mentioned in the outside state above with one small addition. We need to save the normal of the point where the hand touches the surface. When we use Unity's $Rigidbody.SweepTest$ method we gain some information about the object hit, if any. This includes the normal of the point that the sweep with the hand hit. We need this normal for the next part: sliding. Sliding is where we skip movement in a particular direction (and its opposite), effectively moving the hand on a plane. This particular direction is movement along the normal and its opposite. An example of sliding in use is when the hand is interacting with a flat surface and the controller is inside the surface. When moving the controller right or left, for instance, the hand should slide on the plane created by the normal of the contact point, which is approximately the same as sliding across the surface, if the contact point is updated frequently. To implement the sliding we first find the vector between the hand and the target on the controller. This vector is projected onto the plane created with the normal. Moving the hand along this new projected vector reduces the distance to the controller without penetrating the object. As a final step for the touching state, the hand is depenetrated again to make sure it's not inside an object after sliding. To summarize, the touching state rotates the hand and then checks if the hand has now been removed from or placed inside an object. If it has, it first moves it to the touching position again, but then it slides the hand to the position on the surface which is the closest to the controller.

There are two transitions from the touching state back to the open state. The first transition checks if the controller has moved back outside the object compared to the surface the hand is currently touching. The direction from the hand's current position and that of the controller is computed. If the direction is in the same general direction as the normal of the surface\footnote{To determine if the direction vector points in the same general direction as the normal, we use the dot product of the two. If the dot product is positive they are pointing in the same general direction.}, it means that the controller was moved back outside the object in the direction from where it entered. In this case the hand goes to the outside state. The other transition checks if we can sweep with the hand to the controller. If we can sweep all the way without hitting any objects it means that the hand has slid over an edge and the controller is currently free of any objects. Due to how Unity's $Rigidbody.SweepTest$ method is implemented, it's necessary to move the hand slightly away from the object's surface. When the object is touched sweeping doesn't recognize the object being hit. We move the hand slightly back in the direction of the normal of the touch point, before sweeping. In the case we hit something, the hand is still touching a surface and we stay in the touching state. If not the hand transitions into the outside state.

\begin{figure}[H]
\small
\begin{enumerate}
\item Transition to new state, if necessary.
\begin{enumerate}[noitemsep,label=\alph*.]
\item If the last state executed was outside, transition to the touching state if:
\begin{enumerate}[noitemsep,label=\arabic*.]
\item the sweep during the last outside state execution hit an object.
\end{enumerate}
\item If the last state executed was touching, transition to the outside state if:
\begin{enumerate}[noitemsep,label=\arabic*.]
\item the controller was moved away from the currently touched surface (in the general direction of the normal of the contact point), or
\item the sweeping towards the controller didn't hit any object.
\end{enumerate}
\end{enumerate}
\item Execute selected state behaviour.
\end{enumerate}
% first column
\begin{minipage}[t]{0.49\textwidth}
\small
The outside state:
\begin{enumerate}[noitemsep]
\item Orient the hand's rigidbody compared to the target controller.
\item Sweep towards the target controller.
\item If no obstacles in the path between the hand and the controller, move the hand's rigidbody to the target controller.
\item Else move to the position where the sweep hit the obstacle.
\end{enumerate}
\end{minipage}
\hspace{2em}% <---- Don't forget this %
%second column
\begin{minipage}[t]{0.49\textwidth}
\small
The touching state:
\begin{enumerate}[noitemsep]
\item Orient the hand's rigidbody compared to the target controller.
\item Check if inside object and depenetrate if so.
\item If the hand was not inside an object, sweep towards the target controller.
\item If the sweep hit an object, the controller was not touching. Position the hand on the object hit using the sweep information.
\item Use the normal of the contact point to define a plane on which to slide to minimize the distance to the controller.
\item Check if sliding the hand has led it to penetrating an object and depenetrate if so.
\end{enumerate}
\end{minipage}
\caption{Step-by-step process for placing the Sliding rigid hand.}
\label{fig:stepByStepSlidingRigidHand}
\end{figure}

\todo[inline]{Maybe split the sliding rigid hand step-by-step figure into two figures, so it can be split over two pages.}

\todo[inline]{Write about the good and bad cases and pros and cons for this hand prototype. Like a mini-conclusion.}

\subsection{The Finger rigid hand}
\label{subsec:slidingRigidHand}
The Finger rigid hands are based upon the Rigid hand's way of moving and rotating, but tries to improve upon it by adding finger filtering on top. Where the Rigid hand moves and rotates using the rigibody's $MovePosition$ and $MoveRotation$ methods\footnote{If rigidbody interpolation is enabled, using the $MovePosition$ and $MoveRotation$ methods results in a smooth transition in any intermediate frames rendered.} and then depenetrates the hand from possible overlaps with objects in the world, the Finger rigid hand adjusts the fingers in between the placement and the depenetration.

Before continuing with how the Finger rigid hand and its fingers are placed, we need to mention that the animation for this hand is a bit different from the other prototypes. The animation for the Finger rigid hand has a mask for each finger, which means that the fingers can be animated individually even though we use the same animation frames as for the other prototypes, namely one open and one closed hand frame.

As mentioned above the first thing the Finger rigid hand does is to move to the location of the orientation of the controller by using its rigidbody's $MovePosition$ and $MoveRotation$. When the hand has been placed we save the current closed value of each finger. The closed value here isn't necessarily the same as the trigger button value, as this is how closed the finger was after the adjustment last frame. After saving the closed values all the fingers are opened fully using the animation. We need to do this before the next step, which is to calculate the average up vector of all the finger parts for each finger. So for each finger, the average over all its part's up vectors is saved. When the averages have been calculated, we reset the fingers' closed value to the value saved before and the finger positions are updated according to their closed value using the animation. The next step is to depenetrate the hand from any nearby objects, which is done as previously described. This step is important because we're going to raycast from the fingers later and we need to make sure the origin of the raycasts aren't inside any objects we would like to raycast towards\footnote{In Unity raycasts don't hit backfaces of objects.}. 

With all this legwork done, we're now set to start adjusting the fingers. The general idea is that we want to find out how the fingers are oriented according to the surface they are interacting with. There are three cases we consider: If the hand is not near any objects, the finger does not adjust, if the hand is positioned such that bending the finger will not allow it to move away from the surface (the rotation axis is parallel to the surface normal), we skip adjusting the finger and the last case is if the rotation axis is not parallel with the surface normal. In this case we use the difference between the normal and the rotation axis to decide on how to adjust the fingers.

Before we go into the calculations for the individual fingers we first calculate the distance between the hand and the controller. All the fingers use this value to determine how closed or open they need to be. With the distance calculated the next steps are performed per finger. To find out if the finger's rotation axis is parallel with the surface normal, we first find the colliders within a radius of the hand\footnote{Unity's physics API exposes the method $Physics.OverlapSphere$ for finding all colliders within a certain radius from a point.} and find the one closest to the finger. If no collider was found near the hand, we skip the finger adjustment and lerp its closed value towards the value determined by the players input. If a collider was found, we use raycasting to find the normal of the closest point on the collider\footnote{The closest point on a collider can be found using Unity's $Collider.ClosestPoint$ method.}. Since we defined the rotation axis of the finger ourselves and we found the normal of the surface, we can take the dot product of the two to see if they are parallel with each other. If they are, the finger cannot bend away from the surface and like with the case of no nearby colliders the finger's closed value is lerped towards the closed value defined by the player's input. In the final case where the normal and the finger's rotation axis are not parallel, we use the dot product between the normal and the rotation axis and the distance to the controller to find the value to adjust the finger's closed value by. We use two of Unity's animation curves\footnote{Unity's animation curves are functions that can be visually defined and takes an input value and returns an output value.}: One that evaluates the distance and one that evaluates the dot product. The two resulting values are then multiplied with each other, giving us the adjustment for the finger. After calculating the adjustment value for the finger, we set the finger's closed value equal to the sum of the closed value defined by the player's input and the adjustment value. When all the fingers' closed values have been set, they are used to update the animation of the hand along with the animation masks.



\todo[inline]{Write about the good and bad cases and pros and cons for this hand prototype. Like a mini-conclusion.}

\subsection{The Physics hand}
\label{subsec:slidingRigidHand}
The big difference between the Physics hand and the other prototypes is that it is using a non-kinematic rigidbody and is therefore affected by the physics simulation. This means that it can be pushed around by other objects and the physics system will handle collisions. In for this to work the hands need to be moved and rotated differently from the other hands. Instead of setting their position and orientation directly, velocities are applied which makes the hand move and rotate to where they need to go. This means that we calculate what velocity is needed for the hand to move the distance between itself and the controller target and the angular velocity needed for the hand to rotate to the controller orientation. In Unity it's important that these velocities are set in the fixed update method, because they need to be applied before the engine's physics calculations takes place.

\todo[inline]{Reference NewtonVR as the source of inspiration for the Physics hand movement.}

Before applying the velocities described above, we position the fingers of the hand. The fingers of the Physics hand are moved using an inverse kinematics system (also known as an IK system), when they are filtered from the input. Inverse kinematics make use of the kinematics equations to calculate positions for a joint system where we have desired positions for the end effectors. This maps nicely to the movement of fingers, since a finger is a system of joints, where we know the root position of the finger and we'll set the desired location of the end effector, the tip of the finger. The IK system will then calculate how to bend each of the finger's joints to make the tip reach the desired position. To make sure the fingers bend naturally, restrictions can be set for how the joints can bend. The IK system will try to get the tip of the finger's as close to their desired locations without compromising the restrictions for the joints. The restrictions are set up in such a way that the joints can only bend around a single axis which means that they cannot bend sideways, for instance. The axis around which they bend is chosen such that bending the fingers closes the hand into a fist. The axis for the thumb is different from the other fingers, because of its placement on the hand. We have used a Unity asset called Final IK (\textbf{\textit{REFERENCE TO FINAL IK}}), which implements several components and methods that can be used to setup and run IK calculations.

The Physics hand has several states that indicate whether the fingers should filtered or not. The states are named as follows: Free, Fist, Hover, Holding and Lerp to free. When the hand is not near any objects and the player hasn't closed the hand using the trigger button on the controller the hand will be in the Free state. In this state, the fingers will be positioned according to the animation and the trigger value. The trigger value is used to blend between the open and closed frames of the hand's animation. The hand can transition from the Free state into the Fist state, if the player presses the trigger enough for the hand to be considered closed\footnote{As mentioned above, in our prototypes we consider the hand closed when the trigger has been pressed half way, meaning that the trigger value is 0.5 or above.}. In the Fist state the fingers work exactly like in the Free state, dictated by the animation and the trigger value, and besides transitioning back to the Free state, when the player again opens the hand, both the Free and the Fist states can transition to the Hover state. From both states this happens when there is a grabbable object inside the hand's grab area. In addition to this the hand needs to have been opened in the Fist state to transition. A grabbable object is an object which has our grabbable component attached to it. The grabbable component will be described further in section \ref{subsec:grabbingSystem} about our grabbing systems. The grab area is a trigger collider attached to the hand which indicates the range at which the hand can grab objects in the world. When a grabbable object enters the grab area trigger and the object is below the hand, as in the object is on the side the palm of the hand is facing, the hand can enter the Hover state. In the Hover state the finger positions will be filtered in a way that tries to let the fingers approach and touch the object. This filtering will be described further below. From the Hover state the hand can transition to either the Holding state or the Lerp to free state. The Holding state is entered, when the player has grabbed an object. In the Holding state the fingers are not updated, effectively fixating them to their last positions. The hand transitions from the Hover to the Lerp to free state, when there is no grabbable object inside the grab area trigger anymore. The Lerp to free state is inserted in between the Hover and the Free / Fist states to make the transition smooth. In the Lerp to free state the fingers will lerp from their hover positions back to the positions indicated by the animation and trigger button values. When the lerping has finished, the hand will automatically transition to either the Free or the Fist states depending on whether the hand is closed or not. The Free to lerp state can be interrupted if the hand is again hovering over a grabbable object in which case it transitions back to the Hover state.

\missingfigure[figwidth=15cm, figheight=8cm]{Diagram: State change diagram.}

As mentioned above the fingers positions are only filtered in three of the five states: The Holding state, the Hover state, the Lerp to free state. The Free and Fist states don't filter the input but lets the fingers follow the animation and trigger button value directly. In the Holding state the finger positions are filtered by skipping the input completely. The fingers are fixed, while holding objects. The Hover state is where the most interesting filtering happens. It has several steps to find the positions for the fingers. First of all we skip placing the fingers, if the grabbable the hand was hovering over is no longer within the grab area. In the next frame the hand will transition out of the Hover state because of this, as mentioned above. If there is a grabbable object in the grab area we first place the fingers in accordance to the animation and trigger button value to use as the starting position.

What the Hover state tries to achieve is to find suitable positions on the surface of the grabbable object on where to place the fingers. We try three different methods in priority to find the best location for each finger. The first of the three methods is to find the point on the grabbable that is the closest to the fingertip\footnote{Unity's API allows us to find the closest point on a collider from a given point by using the $Collider.ClosestPoint$ method on the collider in question.}. The next method is to find the point on the surface found by moving along the negative up vector of the fingertip, so finding a point on the object that is directly below the fingertip. For this we use raycasting from the fingertip in the direction of the negative up vector of the fingertip. The final method is to find some direction in between the negative up vector of the fingertip and the direction towards the closest point on the grabbable object. In order to find this middle direction we lerp between the vectors for the direction to the closest point and the negative up vector with respects to a middle interpolation value\footnote{This value is a value between 0 and 1, where 0 returns the start lerping vector and 1 returns the end lerping vector. Using different values moves the position found for the fingers in between the two given direction vectors. We use the value 0.86 for our Physics hand.}. The last two methods are not guaranteed to hit the grabbable object at all. The priority of the three methods is as follows; first we see if moving along the direction which is in between the negative up and the direction to the closest point hits a point on the grabbable object. If it does, we use that point as the position the finger needs to move to. If it didn't hit, we try with the negative up from the fingertip and if it hits, we use that hit point as the position to move the finger to. If none of the above methods resulted in a point on the grabbable object then the closest point on the object is used as the target for the finger. The reason for this prioritization has to do with certain placements of the hand compared to the grabbable object and the grabbable object's shape. If the closest point was always used and the hand was hovering over a grabbable cube with the palm on one of the surfaces and the fingers sticking out over the edge, the result would lead to the two furthest joints in the fingers both bending about 90 degrees without the joint between the fingers and the palm bending much, which is not a natural pose and is therefore not ideal (\textbf{\textit{IMAGE REQUIRED TO SHOW EXAMPLE WITH CUBE}}). The same will occur when hovering over thin objects like sticks or bats. The closest position on the grabbable from the fingertips will be close to the palm resulting in the weird pose. Using the negative up vector of the fingertips will also not work in a lot of cases. If we again look at the example with the grabbable cube. If the hand is completely open and the fingers are sticking out over the edge, moving along the negative up vector will not lead to a hit on the cube. Because of this, having a middle point between the closest point and the negative up can help in these cases by bending the fingers towards the grabbable object, but with a more natural pose than what results from the closest point. When the positions for the fingertips have been found by using one of these methods we lerp the IK targets of the fingers towards the found positions, as to smooth the movement to the found positions, after which we call the IK system to solve for the new position.

\begin{figure}[H]
\centering
\small
% POSITION AND ROTATION.
\begin{flushleft}
Position and rotation:
\end{flushleft}
\begin{enumerate}[noitemsep]
\item Calculate the angular velocity needed to rotate the hand to the orientation of the controller in this frame and apply it to the hand's rigidbody.
\item Calculate the velocity needed to move the hand to the position of the controller in this frame and apply it to the hand's rigidbody.
\end{enumerate}
\caption{Step-by-step process for placing the Physics hand.}
\label{fig:stepByStepPhysicsHandPositionRotation}
\end{figure}

\begin{figure}[H]
\centering
\small
% FINGER POSITIONS.
\begin{flushleft}
Finger positions:
\end{flushleft}
\begin{itemize}[noitemsep]
\item If in the Hover state:
\begin{enumerate}[noitemsep]
\item Check if there is a grabbable object in the grab area of the hand below the palm. If not, skip the rest.
\item Move all fingers to the location indicated by the animation and the trigger button value as a starting position.
\item For each finger, get the negative up vector from the fingertip.
\item Get the closest position from the fingertip to the grabbable object and create a direction vector starting from the fingertip and pointing at the closest point.
\item Calculate the middle vector as a vector which resides between the negative up and the closest point direction. This can be done by lerping. The amount by which to lerp is a variable which needs to be tweaked.
\item Raycast towards the middle direction with the max distance being the radius of the grab area. If the grabbable object was hit, set the hit point as the finger's target.
\item If the previous raycast didn't hit the object, raycast in the negative up direction with the same distance. If the object was hit, set the hit point as the finger's target.
\item If none of the raycasts hit, use the closest point on the grabbable object as the finger's target.
\item Set all fingers to their open position using the animation as a base. This is to start the IK solving from the same location every frame.
\item Lerp the finger's IK targets towards the calculated finger targets.
\item Let the IK system solve for the new IK targets and move the fingers.
\end{enumerate}
\item If in the Lerp to free state:
\begin{enumerate}[noitemsep]
\item For each finger, save its current IK target position.
\item Get the position that each finger would have according the the animation and trigger button value.
\item Find an intermediate position for the fingers slerping\footnote{SOMETHING ABOUT SLERPING HERE!} from the saved IK target position towards the animation position.
\item Set the IK target of the finger to the intermediate position found for it.
\end{enumerate}
\item If in the Holding state:
\begin{enumerate}[noitemsep]
\item Skip all finger movement.
\end{enumerate}
\item All other states:
\begin{enumerate}[noitemsep]
\item Set each finger's IK target to the position indicated by the animation and trigger button values.
\end{enumerate}
\end{itemize}
\caption{Step-by-step process for placing the Physics hand's fingers.}
\label{fig:stepByStepPhysicsHandFingers}
\end{figure}

\todo[inline]{Update the step-by-step figure to include the fingers as well.}

The obvious benefit gained from using Unity's physics system to handle collisions is that they work more consistently across different cases like flat surfaces, non-flat surfaces, edges, corners and more and each case is handled by Unity instead of by our own code. We also gain other benefits from using the physics system. One of them is the ability to lift objects without having to grab them using a grabbing mechanism activated by for instance pressing the trigger on the controller\footnote{The different grabbing and lifting systems will be explained in section \ref{subsec:grabbingSystem}.}. Another benefit is the ability to get a feeling of weight, when interacting with objects. Often in VR objects of different mass feel indistinguishable from one another, when interacting with them. Except for the visual feedback of size, it's often impossible to gain an impression of how heavy an object should be. Letting the physics system handle interactions with objects allows the hands to portray a sense of mass, by using the mass of both the hand and the object pushed or lifted when calculating the forces applied.

\todo[inline]{Write the mini-conclusion for the physics hand. Include the bad case of the rotation that jumps when the distance to the controller becomes too great. Moving fingers affect rigidbody and makes it move.}

\subsection{The Rotation hand}
\label{subsec:slidingRigidHand}
The Rotation hand is the only prototype we implemented that explicitly filters on all three variables: position, rotation and finger positions. It has two modes of movement. First, we check if there are no objects in the hand's vicinity. We can do this by using Unity's physics API which exposes the method $Physics.OverlapSphere$ for finding all colliders within a certain radius from a point. If there are no objects nearby the position and orientation of the hand is set to the position and orientation of the controller. If there are object nearby we act upon the closest collider. We first want to rotate the hand. The goal of the explicit rotation filtering is to make the palm face the surface of the object, when the hand is open and make the front of the hand face the surface, when the hand is closed into a fist (\textbf{\textit{IMAGES HERE?}}). The rotation should be more or less complete depending on the distance to the object meaning that if the hand is touching the object, the palm should be facing directly towards the surface and if the hand is far away it should orient itself like the controller target, but blend when in-between (\textbf{\textit{CREATE IMAGE SEQUENCE}}). The hands orientation is first set equal to that of the controller target. This is used as the starting position from where we apply additional rotation. To calculate the additional rotation to apply to the hand, when closing in on an object, the normal of the object's surface is required. Depending on if the hand is open or closed we also want the palm's up vector or negative forward vector respectively. In the case of the open hand, we want the palm's up vector to rotate towards the normal of the surface, when the distance is reduced. Unity provides a lerp method to linearly interpolate between two vectors\footnote{In Unity the $Vector3.Lerp$ method can be used to linearly interpolate between two vectors.}, which is used to find the direction vector to which the palm's up should point depending on the current distance to the surface. To increase the amount of control we have over the rotation, don't use the distance directly. We first pass it through a function which we define using one of Unity's animation curves. This way we can control if the hand should rotate faster between certain distances, for instance. When the direction vector has been calculated, we use it together with the current up vector (or negative forward in the case of the hand being closed) to define a rotation\footnote{The rotation between two vectors can be calculated using Unity's $Quaternion.FromToRotation$ method.}, which we add to the hand's current rotation.

When rotation has occurred it's time to move the hand. In order for the hand not to penetrate the object it's moving towards, we sweep from the hand towards the controller position. When sweeping, we don't want the fingers to be taken into account. The accommodate for this, we don't have colliders on the fingers except for the finger tips. These colliders are disabled at all times, except during the calculation of the finger positions, which is described below. As mentioned before, Unity's $Rigidbody.SweepTest$ method doesn't detect objects that the hand is already touching, so we move it back slightly in the direction of the surface normal before sweeping. If the sweep hit an object the hand is moved to the hit position after which its slid across the surface to reduce the distance to the controller position. The sliding works in the same way as for the Sliding rigid hand (Section \ref{subsec:slidingRigidHand}). The normal is used to create a plane upon which the vector which spans between the hand position and the controller position is projected upon. This new vector is then what determines where to move the hand. If no object was hit during the sweep test, the hand is moved straight to the controller position.

The fingers of the Rotation hand are moved using an inverse IK which is set up for this hand like described for the Physics hand above. When we want to move the fingers, there are two contexts we consider; either the hand is near objects that the fingers might interact with or it isn't. In both cases we start off by setting the fingers' targets to the positions defined by the controller's trigger value. This position is gathered from the hand animation, which is blended between an open and closed animation frame. In the case that there aren't any nearby objects nothing more is done to the fingers. When the player presses the trigger they will therefore close the hand. In the other case, where the hand is close to one or more objects, the fingers will be adjusted so that they don't penetrate any objects. To do this, all colliders within a radius are found using the same method described above. For each of the found colliders we check if the finger tips are inside by computing the penetration between the finger tip collider and the found collider. Before this step we enable the finger tip colliders, which are usually disabled. After computing the penetration we disable the finger tip collider again. The distance and direction gathered from computing the penetration is then used to move the finger's IK target. This means that the desired location the finger is trying to move to is moved outside of the object.

\begin{figure}[H]
\small
Position and rotation:
\begin{enumerate}[noitemsep]
\item If no objects are within a radius from the hand, move towards the position and the rotation of the controller and skip the rest.
\item Else find the orientation to rotate towards:
\begin{enumerate}[noitemsep]
\item Get the normal of the closest point on the closest object.
\item Get the current palm direction vector as the palm's up vector if the hand is open and the palm's negative forward vector if the hand is closed.
\item Calculate target direction vector by lerping between the current palm direction vector and the normal vector.
\item Calculate the rotation between the current palm direction vector and the target direction vector.
\item Rotate the hand towards the sum of the controller orientation and the rotation calculated in the previous step.
\end{enumerate}
\item And then move the hand:
\begin{enumerate}[noitemsep]
\item Move the hand a tiny bit in the direction of the normal (so the hand is not touching the object, when sweeping).
\item Sweep the hand towards the controller position.
\item If the sweep hit an object, place hand at hit point and slide to minimize the distance between the hand and the controller.
\end{enumerate}
\end{enumerate}
Finger positions:
\begin{enumerate}[noitemsep]
\item Set each finger's IK target to the animation's finger tip position.
\item Find all colliders within a radius from the hand.
\item If no colliders were found, skip the rest.
\item Else enable finger tip colliders.
\item Compute each finger tip collider's penetration with the colliders found in step 2.
\item Move each finger's IK target using the distance and direction found by computing the tip's penetration.
\item Disable the finger tip colliders again.
\end{enumerate}
\caption{Step-by-step process for placing the Rotation hand and its fingers.}
\label{fig:stepByStepRotationHand}
\end{figure}

\todo[inline]{Write the mini-conclusion for the Rotation hand}

\section{Hand visualization}
\label{subsec:handVisualization}

\section{Rumble}
\label{subsec:RUMLBEZ}

\todo[inline]{Perhaps merge the hand visualization and rumble sections into one about enhancements to the experience or something.}

\section{The grabbing system}
\label{subsec:grabbingSystem}
